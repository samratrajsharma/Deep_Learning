{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07719ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains CGAN on MNIST using Keras\n",
    "# Conditional GAN - z vector is conditioned by a one-hot label\n",
    "# Based on DCGAN structure\n",
    "\n",
    "from tensorflow.keras.layers import Activation, Dense, Input, Conv2D, Flatten, Reshape, Conv2DTranspose, LeakyReLU, BatchNormalization, concatenate\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Generator model\n",
    "def build_generator(inputs, labels, image_size):\n",
    "    image_resize = image_size // 4\n",
    "    kernel_size = 5\n",
    "    layer_filters = [128, 64, 32, 1]\n",
    "\n",
    "    x = concatenate([inputs, labels], axis=1)\n",
    "    x = Dense(image_resize * image_resize * layer_filters[0])(x)\n",
    "    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\n",
    "\n",
    "    for filters in layer_filters:\n",
    "        strides = 2 if filters > layer_filters[-2] else 1\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
    "\n",
    "    x = Activation('sigmoid')(x)\n",
    "    return Model([inputs, labels], x, name='generator')\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator(inputs, labels, image_size):\n",
    "    kernel_size = 5\n",
    "    layer_filters = [32, 64, 128, 256]\n",
    "\n",
    "    y = Dense(image_size * image_size)(labels)\n",
    "    y = Reshape((image_size, image_size, 1))(y)\n",
    "    x = concatenate([inputs, y])\n",
    "\n",
    "    for filters in layer_filters:\n",
    "        strides = 1 if filters == layer_filters[-1] else 2\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    return Model([inputs, labels], x, name='discriminator')\n",
    "\n",
    "# Training function\n",
    "def train(models, data, params):\n",
    "    generator, discriminator, adversarial = models\n",
    "    x_train, y_train = data\n",
    "    batch_size, latent_size, train_steps, num_labels, model_name = params\n",
    "    save_interval = 500\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
    "    noise_class = np.eye(num_labels)[np.arange(0, 16) % num_labels]\n",
    "    train_size = x_train.shape[0]\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        # Train discriminator\n",
    "        rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
    "        real_images = x_train[rand_indexes]\n",
    "        real_labels = y_train[rand_indexes]\n",
    "\n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "        fake_labels = np.eye(num_labels)[np.random.choice(num_labels, batch_size)]\n",
    "        fake_images = generator.predict([noise, fake_labels])\n",
    "\n",
    "        x = np.concatenate((real_images, fake_images))\n",
    "        labels = np.concatenate((real_labels, fake_labels))\n",
    "        y = np.ones([2 * batch_size, 1])\n",
    "        y[batch_size:, :] = 0.0\n",
    "\n",
    "        d_loss, d_acc = discriminator.train_on_batch([x, labels], y)\n",
    "\n",
    "        # Train adversarial (freeze discriminator)\n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "        fake_labels = np.eye(num_labels)[np.random.choice(num_labels, batch_size)]\n",
    "        y = np.ones([batch_size, 1])\n",
    "        a_loss, a_acc = adversarial.train_on_batch([noise, fake_labels], y)\n",
    "\n",
    "        print(f\"{i}: [D loss: {d_loss:.4f}, acc: {d_acc:.4f}] [A loss: {a_loss:.4f}, acc: {a_acc:.4f}]\")\n",
    "\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            plot_images(generator, noise_input, noise_class, step=(i + 1), model_name=model_name)\n",
    "\n",
    "    generator.save(model_name + \".h5\")\n",
    "\n",
    "# Plot generated images\n",
    "def plot_images(generator, noise_input, noise_class, step=0, model_name=\"cgan\"):\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "    filename = os.path.join(model_name, f\"{step:05d}.png\")\n",
    "    images = generator.predict([noise_input, noise_class])\n",
    "    plt.figure(figsize=(2.2, 2.2))\n",
    "    rows = int(math.sqrt(noise_input.shape[0]))\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(rows, rows, i + 1)\n",
    "        plt.imshow(images[i].reshape(images.shape[1], images.shape[2]), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Main logic\n",
    "def main():\n",
    "    (x_train, y_train), (_, _) = mnist.load_data()\n",
    "    image_size = x_train.shape[1]\n",
    "    x_train = np.reshape(x_train, [-1, image_size, image_size, 1]).astype('float32') / 255\n",
    "    y_train = to_categorical(y_train)\n",
    "    num_labels = y_train.shape[1]\n",
    "    latent_size = 100\n",
    "    batch_size = 64\n",
    "    train_steps = 10000\n",
    "    model_name = \"cgan_mnist\"\n",
    "\n",
    "    inputs = Input(shape=(latent_size,))\n",
    "    labels = Input(shape=(num_labels,))\n",
    "    generator = build_generator(inputs, labels, image_size)\n",
    "\n",
    "    image = Input(shape=(image_size, image_size, 1))\n",
    "    labels_d = Input(shape=(num_labels,))\n",
    "    discriminator = build_discriminator(image, labels_d, image_size)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=0.0002), metrics=['accuracy'])\n",
    "\n",
    "    discriminator.trainable = False\n",
    "    fake_image = generator([inputs, labels])\n",
    "    fake = discriminator([fake_image, labels])\n",
    "    adversarial = Model([inputs, labels], fake)\n",
    "    adversarial.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=0.0002), metrics=['accuracy'])\n",
    "\n",
    "    models = (generator, discriminator, adversarial)\n",
    "    data = (x_train, y_train)\n",
    "    params = (batch_size, latent_size, train_steps, num_labels, model_name)\n",
    "    train(models, data, params)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
